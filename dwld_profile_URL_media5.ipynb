{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364756a-a6c9-407e-94ab-f48e7bcdd623",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#Download tweets \n",
    "#First look for lines saying: os.environ['TOKEN'] = '' and write you bearer token\n",
    "# For sending GET requests from the API\n",
    "import requests\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "#To add wait time between requests\n",
    "import time\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c133dc-f1c3-4fc3-8f58-2c8888e6b121",
   "metadata": {},
   "source": [
    "###################### Replace your bearer token below within the quotes and and look for other line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeaaa47-bd8a-4d32-8af8-62f72dc86553",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKEN'] = 'REPLACEYOUR bearer_token HERE' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4483d79-6c85-4dd6-abb7-084dc5743c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732a941-a739-4872-a15c-85212e3675a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d15776-ec6b-44ff-8d68-c1a094325fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth():\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def create_url(keyword, start_date, end_date, max_results = 10):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\" \n",
    "\n",
    "    \n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,referenced_tweets.id,referenced_tweets.id.author_id,entities.mentions.username,attachments.poll_ids,attachments.media_keys,in_reply_to_user_id,geo.place_id',\n",
    "                    \n",
    "                    'tweet.fields': 'attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,possibly_sensitive,public_metrics,referenced_tweets,reply_settings,source,text',\n",
    "                    'media.fields': 'duration_ms,height,media_key,preview_image_url,public_metrics,type,url,width,alt_text',\n",
    "                    'user.fields': 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)  \n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def append_csv_url(json_response, fileName, username):\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    user_username=[]\n",
    "    erors=[]\n",
    "\n",
    "    resp =json_response['includes']\n",
    "\n",
    "    for i in resp['users']:\n",
    "        usernames= i.get('username',\"None\")\n",
    "        user_username.append(usernames)\n",
    "\n",
    "    cc=0\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "\n",
    "\n",
    "        if('conversation_id' in tweet):\n",
    "            conversation_id = tweet['conversation_id']\n",
    "        else:\n",
    "            conversation_id= ''\n",
    "        \n",
    "        \n",
    "        conversation_url1 = f\"https://twitter.com/{user_username[cc]}/status/{conversation_id}\"\n",
    "                # 9. attachments\n",
    "\n",
    "        # Assemble all data in a list\n",
    "        appli= [username, conversation_url1]#, erors[cc]]\n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(appli)\n",
    "        cc +=1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "\n",
    "def append_csv_app(json_response, fileName, username):\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    media_url=[]\n",
    "    count_media=0\n",
    "    user_username=[]\n",
    "\n",
    "    resp =json_response['includes']\n",
    "    \n",
    "    if 'media' in resp:\n",
    "        for i in resp['media']:       \n",
    "            mur= i.get('url', \"None\")\n",
    "            print(mur)\n",
    "            media_url.append(mur) \n",
    "    for i in resp['users']:\n",
    "        usernames= i.get('username',\"None\")\n",
    "        user_username.append(usernames)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    cc=0\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "\n",
    "        if('conversation_id' in tweet):\n",
    "            conversation_id = tweet['conversation_id']\n",
    "        else:\n",
    "            conversation_id= ''\n",
    "        \n",
    "        conversation_url = f\"https://twitter.com/{username}/status/{conversation_id}\"\n",
    "        conversation_url1 = f\"https://twitter.com/{user_username[cc]}/status/{conversation_id}\"\n",
    "                # 9. attachments\n",
    "        if ('attachments' in tweet): \n",
    "            attachments =tweet['attachments']\n",
    "            m_url=media_url[count_media]\n",
    "            count_media +=1\n",
    "            print(count_media)\n",
    "        else:\n",
    "            attachments = \" \"\n",
    "            m_url= \" \"\n",
    "        ################Agregando Users information\n",
    "         \n",
    "        # Assemble all data in a list\n",
    "        appli= [username, like_count, quote_count, reply_count, retweet_count, text,  conversation_url1, m_url]\n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(appli)\n",
    "        cc +=1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "######################################\n",
    "\n",
    "\n",
    "def append_to_csv(json_response, fileName, username):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "    cc=0\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    media_url=[]\n",
    "    res_app=[]\n",
    "    count_media=0\n",
    "    user_username=[]\n",
    "\n",
    "    resp =json_response['includes']\n",
    "\n",
    "    if 'media' in resp:\n",
    "        for i in resp['media']:        \n",
    "            mur= i.get('url', \"None\")\n",
    "            print(mur)\n",
    "            media_url.append(mur) \n",
    "    \n",
    "    for i in resp['users']:\n",
    "        usernames= i.get('username',\"None\")\n",
    "        user_username.append(usernames)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "       \n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Geolocation\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "\n",
    "        # 4. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 7. source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "\n",
    "        # 9. attachments\n",
    "        if ('attachments' in tweet): \n",
    "            attachments =tweet['attachments']#\n",
    "            m_url=media_url[count_media]\n",
    "            count_media +=1\n",
    "            print(count_media)\n",
    "        else:\n",
    "            attachments = \" \"\n",
    "            m_url= \" \"\n",
    "\n",
    "        #10 possibly_sensitive\n",
    "        if ('possibly_sensitive' in tweet): \n",
    "            possibly_sensitive= tweet['possibly_sensitive']\n",
    "        else:\n",
    "            possibly_sensitive = 'False'\n",
    "\n",
    "        if('context_annotations' in tweet):\n",
    "            context_annotations= tweet['context_annotations']\n",
    "        else:\n",
    "            context_annotations= ''\n",
    "\n",
    "        if('conversation_id' in tweet):\n",
    "            conversation_id = tweet['conversation_id']\n",
    "        else:\n",
    "            conversation_id= ''\n",
    "\n",
    "\n",
    "        if('entities' in tweet):\n",
    "            entities = tweet['entities']\n",
    "        else:\n",
    "            entities= ''\n",
    "\n",
    "\n",
    "        if('in_reply_to_user_id' in tweet):\n",
    "            in_reply_to_user_id= tweet['in_reply_to_user_id']\n",
    "        else:\n",
    "            in_reply_to_user_id= ''     \n",
    "\n",
    "\n",
    "        if('referenced_tweets' in tweet):\n",
    "            referenced_tweets= tweet['referenced_tweets']\n",
    "        else:\n",
    "            referenced_tweets= ''  \n",
    "\n",
    "\n",
    "        if('reply_settings' in tweet):\n",
    "            reply_settings= tweet['reply_settings']\n",
    "        else:\n",
    "            reply_settings= ''     \n",
    "\n",
    "        \n",
    "        conversation_url= f\"https://twitter.com/{user_username[cc]}/status/{conversation_id}\"\n",
    "        ################Agregando Users information\n",
    "   \n",
    "        \n",
    "        # Assemble all data in a list\n",
    "\n",
    "        res = [attachments, author_id, context_annotations, conversation_id, created_at, entities, geo, tweet_id, in_reply_to_user_id, lang, possibly_sensitive,referenced_tweets, reply_settings, source, like_count, quote_count, reply_count, retweet_count, text, conversation_url,  username, m_url]#,, profile_image] #, profile_image_url]#, user_id,user_name,user_username,user_description, user_retweet_count, user_reply_count,user_like_count, user_quote_count]\n",
    "        appli= [username, like_count, quote_count, reply_count, retweet_count, text, conversation_url, m_url]\n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "        \n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e14f9-6f03-4ff5-8012-0eb6169ab509",
   "metadata": {},
   "source": [
    "Replace your bearer token below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e884d-6126-4f37-b519-70e1234bf03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Inputs for tweets\n",
    "    os.environ['TOKEN'] = 'REPLACEYOUR bearer_token HERE' \n",
    "    \n",
    "    ##############\n",
    "    ##############\n",
    "    ############## Variable for candidates' accounts\n",
    "    username = ['harrisonjaime']#, 'hiral4congress','ThomTillis','RepJahanaHayes', 'Meg4Congress','PauletteEJordan']\n",
    "    \n",
    "    fileURL= 'Mentions_URL.csv'\n",
    "    '''csvF_URL=open(fileURL,\"a\", newline=\"\", encoding='utf-8')\n",
    "    urlWriter = csv.writer(csvF_URL)\n",
    "    urlWriter.writerow(['Candidate', 'Conversation_URL'])\n",
    "    csvF_URL.close()'''\n",
    "\n",
    "    for name in username:\n",
    "        ######################\n",
    "        ######################\n",
    "        ######################Variable for the filter\n",
    "        keyword = '(from:'+name+' OR @'+name+') -is:retweet' \n",
    "        file_name= name+'_1profileNRT.csv'\n",
    "        file_json= name+'_1profileNRT.json'\n",
    "        file2_json= name+'_4app.csv'\n",
    "       \n",
    "        bearer_token = auth()\n",
    "        headers = create_headers(bearer_token)\n",
    "\n",
    "#####################\n",
    "#####################\n",
    "#####################Variable for time-windows. start_list and end_list must have the same number of elements\n",
    "        start_list =    ['2020-10-04T15:21:00.000Z',\n",
    "                        '2020-10-11T22:50:00.000Z',\n",
    "                        '2020-10-15T14:30:00.000Z',\n",
    "                        '2020-10-16T13:08:00.000Z', \n",
    "                        '2020-10-18T22:11:00.000Z',\n",
    "                        '2020-11-01T14:35:00.000Z',\n",
    "                        '2020-11-05T19:41:00.000Z',\n",
    "                        '2020-11-08T11:32:00.000Z', \n",
    "                        '2020-11-09T23:12:00.000Z', \n",
    "                        '2020-11-20T21:44:00.000Z']\n",
    "                         \n",
    "\n",
    "        end_list =      ['2020-10-04T15:22:00.000Z',\n",
    "                        '2020-10-11T22:51:00.000Z', \n",
    "                        '2020-10-15T14:31:00.000Z', \n",
    "                        '2020-10-16T13:09:00.000Z',\n",
    "                        '2020-10-18T22:12:00.000Z',\n",
    "                        '2020-11-01T14:36:00.000Z',\n",
    "                        '2020-11-05T19:42:00.000Z',\n",
    "                        '2020-11-08T11:33:00.000Z',\n",
    "                        '2020-11-09T23:13:00.000Z',\n",
    "                        '2020-11-20T21:45:00.000Z']\n",
    "        max_results = 99\n",
    "        \n",
    "        #Total number of tweets we collected from the loop\n",
    "        total_tweets = 0\n",
    "\n",
    "        # Create file\n",
    "        \n",
    "        csvFile = open(file_name, \"a\", newline=\"\", encoding='utf-8')\n",
    "        csvWriter = csv.writer(csvFile)\n",
    "        #Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "                            \n",
    "        csvWriter.writerow(['attachments','author id', 'context_annotations', 'conversation_id','created_at', 'entities','geo', 'tweet_id','in_reply_to_user_id','lang','possibly_sensitive', 'referenced_tweets', 'reply_settings','source', 'like_count', 'quote_count', 'reply_count','retweet_count', 'tweet', 'conversation_url ', 'name','media_url'])\n",
    "        csvFile.close()\n",
    "\n",
    "        csvFile = open(file2_json, \"a\", newline=\"\", encoding='utf-8')\n",
    "        csvWriter = csv.writer(csvFile)\n",
    "        csvWriter.writerow(['username', 'like_count', 'quote_count', 'reply_count', 'retweet_count', 'text', 'conversation_url', 'm_url'])\n",
    "        csvFile.close()\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(0,len(start_list)):\n",
    "\n",
    "            # Inputs\n",
    "            count = 0 # Counting tweets per time period\n",
    "            \n",
    "            flag = True\n",
    "            next_token = None\n",
    "            tweets_list=pd.DataFrame()\n",
    "    \n",
    "            # Check if flag is true\n",
    "            while flag:\n",
    "            \n",
    "                print(\"-------------------\")\n",
    "                print(\"Token: \", next_token)\n",
    "                print(name)\n",
    "                url = create_url(keyword, start_list[i],end_list[i], max_results)\n",
    "                json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "                \n",
    "                result_count = json_response['meta']['result_count']\n",
    "\n",
    "                if 'next_token' in json_response['meta']:\n",
    "                # Save the token to use for next call\n",
    "                    next_token = json_response['meta']['next_token']\n",
    "                    print(\"Next Token: \", next_token)\n",
    "                    if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                        print(\"Start Date: \", start_list[i])\n",
    "                   \n",
    "                        append_to_csv(json_response, file_name, name)\n",
    "                        append_csv_app(json_response, file2_json, name)\n",
    "                        append_csv_url(json_response, fileURL, name)\n",
    "                        \n",
    "                        s=json.dumps(json_response, indent=4)\n",
    "                        f=open(file_json,\"w\")\n",
    "                        f.write(s)\n",
    "                        count += result_count\n",
    "                        total_tweets += result_count\n",
    "                        print(\"Total # of Tweets added: \", total_tweets)\n",
    "                        print(\"-------------------\")\n",
    "                        time.sleep(2)   \n",
    "                                    \n",
    "                # If no next token exists\n",
    "                else:\n",
    "                    if result_count is not None and result_count > 0:\n",
    "                        print(\"-------------------\")\n",
    "                        print(\"Start Date: \", start_list[i])\n",
    "                        \n",
    "                        append_to_csv(json_response, file_name, name)\n",
    "                        append_csv_app(json_response, file2_json, name)\n",
    "                        append_csv_url(json_response, fileURL, name)\n",
    "                        s=json.dumps(json_response, indent=4)\n",
    "                        f=open(file_json,\"w\")\n",
    "                        f.write(s)\n",
    "                        count += result_count\n",
    "                        total_tweets += result_count\n",
    "                        print(\"Total # of Tweets added: \", total_tweets)\n",
    "                        print(\"-------------------\")\n",
    "                        time.sleep(2)\n",
    "                        \n",
    "            \n",
    "                    \n",
    "                    flag = False\n",
    "                    next_token = None\n",
    "                \n",
    "                time.sleep(2)\n",
    "\n",
    "        print(\"Total number of results: \", total_tweets)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d72184-35df-4f60-be48-eeed3b63fec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b91dc8-3389-4e34-9990-8a1738d4cec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
